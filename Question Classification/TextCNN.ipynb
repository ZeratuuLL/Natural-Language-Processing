{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will try TextCNN to classify the questions. Unlike the original paper which used two layers of text embeddings, I only used the non-static one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import jieba\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "roots = {'history' : './data/百度题库/高中_历史/origin/', \n",
    "         'geology' : './data/百度题库/高中_地理/origin/',\n",
    "         'politics' : './data/百度题库/高中_政治/origin/',\n",
    "         'biology' : './data/百度题库/高中_生物/origin/'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stop_words(path):\n",
    "    file = open(path, 'r', encoding='utf-8')\n",
    "    stopwords = file.readlines()\n",
    "    stopwords = [word.strip() for word in stopwords]\n",
    "    return stopwords\n",
    "\n",
    "stopwords = load_stop_words('./stopwords/stopwords2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(root):\n",
    "    '''\n",
    "    This function reads in all csv files lies directly under the root directory\n",
    "    \n",
    "    Returns the file directories as well as class names (file names)\n",
    "    '''\n",
    "    file_names = os.listdir(root)\n",
    "    file_names = [name for name in file_names if name.endswith('csv')]\n",
    "    classes = [name.split('.')[0] for name in file_names]\n",
    "    file_names = [root + name for name in file_names]\n",
    "    datasets = [pd.read_csv(name) for name in file_names]\n",
    "    return datasets, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_line(line):\n",
    "    '''\n",
    "    This function cleans the context\n",
    "    '''\n",
    "    line = re.sub(\n",
    "            \"[a-zA-Z0-9]|[\\s+\\-\\|\\!\\/\\[\\]\\{\\}_,.$%^*(+\\\"\\')]+|[:：+——()?【】《》“”！，。？、~@#￥%……&*（）]+|题目\", '',line)\n",
    "    tokens = jieba.cut(line, cut_all=False)\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(root):\n",
    "    \n",
    "    datasets, classes = read_files(root)\n",
    "    \n",
    "    for dataset, label in zip(datasets, classes):\n",
    "        dataset['item'] = dataset['item'].apply(lambda x:clean_line(x))\n",
    "        dataset['label'] = label\n",
    "    \n",
    "    dataset = pd.concat(datasets, ignore_index = True)\n",
    "    dataset = dataset[['item', 'label']]\n",
    "        \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextCNN\n",
    "\n",
    "Here the parameters that can be tuned about network structure are : \n",
    "    * the number of CNN filters and the kernel size of each of them\n",
    "    * dropout rate\n",
    "    * Embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, window_size_list, word_size, num_classes, pad_token, dropout_rate = 0.1, embedding_size = 300):\n",
    "        \n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(word_size, embedding_size, pad_token)\n",
    "        self.CNN_list = []\n",
    "        for window_size in window_size_list:\n",
    "            self.CNN_list.append(nn.Conv2d(1, 1, (window_size, embedding_size)))\n",
    "        self.fc = nn.Linear(len(window_size_list), num_classes)\n",
    "        self.output = nn.LogSoftmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(p = dropout_rate)\n",
    "        \n",
    "    def forward(self, sentences):\n",
    "        \n",
    "        embedded = self.embedding(sentences)\n",
    "        embedded = embedded.unsqueeze(1) # add in_channel into shape\n",
    "        \n",
    "        feature_list = []\n",
    "        for cnn_layer in self.CNN_list:\n",
    "            features = cnn_layer(embedded) # the last dimension should be 1\n",
    "            features = features.squeeze() # remove the channel dimension and the last dimension\n",
    "            features = torch.tanh(features) # activation layer\n",
    "            features, _ = features.max(dim = -1) # MaxPooling\n",
    "            feature_list.append(features)\n",
    "            \n",
    "        features = torch.stack(feature_list, dim = -1) # now shape is bs, max_window_size\n",
    "        features = self.dropout(features) # dropout for normalization\n",
    "        \n",
    "        logits = self.fc(features)\n",
    "        logits = self.output(logits)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_proc(sentence, max_len, word2id):\n",
    "    \n",
    "    if len(sentence) > max_len:\n",
    "        sentence = sentence[:max_len]\n",
    "    else:\n",
    "        sentence += ['<PAD>'] * (max_len - len(sentence))\n",
    "        \n",
    "    sentence = [word2id.get(word, word2id['<OOV>']) for word in sentence]\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_pad_words(texts, max_feature):\n",
    "    \n",
    "    word_list = [word for sentence in texts for word in sentence]\n",
    "    counter = Counter(word_list)\n",
    "    counter = [(word, count) for word, count in counter.items()]\n",
    "    counter.sort(key = lambda x : x[1], reverse = True)\n",
    "    \n",
    "    valid_words = [word for word, _ in counter[:max_feature]]\n",
    "    word2id = dict(zip(valid_words, range(1, len(valid_words) + 1) ) )\n",
    "    word2id['<OOV>'] = 0\n",
    "    word2id['<PAD>'] = len(word2id)\n",
    "    \n",
    "    lens = [len(sentence) for sentence in texts]\n",
    "    max_len = int(np.mean(lens) + 2 * np.std(lens))\n",
    "    \n",
    "    texts = [sentence_proc(sentence, max_len, word2id) for sentence in texts]\n",
    "    \n",
    "    return texts, word2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_TextCNN(subject, window_size_list, dropout_rate, NGramRange=1, max_feature=10000, embedding_size = 300, epoch = 20):\n",
    "    \n",
    "    print('Reading Data')\n",
    "    root = roots[subject]\n",
    "    dataset = build_dataset(root)\n",
    "    num_topics = len(dataset['label'].unique())\n",
    "    dataset['item'] = dataset['item'].apply(lambda x:x.split())\n",
    "    common_texts=dataset['item'].tolist()\n",
    "    \n",
    "    print('Cleaning Data')\n",
    "    common_texts, word2id = filter_pad_words(common_texts, max_feature)\n",
    "    \n",
    "    TextCNN = Network(window_size_list, len(word2id), num_topics, len(word2id)-1, dropout_rate = dropout_rate).to(device)\n",
    "    TextCNN.train()\n",
    "    optimizer = optim.Adam(TextCNN.parameters(), 0.01)\n",
    "    \n",
    "    print('Creating training/testing set')\n",
    "    label2id = dict(zip(dataset['label'].unique(), range(num_topics)))\n",
    "    id2label = dict(zip(label2id.values(), label2id.keys()))\n",
    "    X = np.array(common_texts)\n",
    "    y = np.array([label2id[label] for label in dataset['label']]).reshape(-1, 1)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                        y, \n",
    "                                                        test_size = 0.2, \n",
    "                                                        random_state = 101)\n",
    "    \n",
    "    X_train = torch.tensor(X_train).long()\n",
    "    y_train = torch.tensor(y_train).long()\n",
    "    X_test = torch.tensor(X_test).long()\n",
    "    y_test = torch.tensor(y_test).long()\n",
    "    train = TensorDataset(X_train, y_train)\n",
    "    test = TensorDataset(X_test, y_test)\n",
    "    train_loader = DataLoader(train, 64, True)\n",
    "    test_loader = DataLoader(test, 64, False)\n",
    "    \n",
    "    print('Training\\n')\n",
    "    criterion = nn.NLLLoss()\n",
    "    for i in range(1, epoch + 1):\n",
    "        \n",
    "        log = []\n",
    "        \n",
    "        for X_sample, y_sample in iter(train_loader):\n",
    "            \n",
    "            X_sample = X_sample.to(device)\n",
    "            y_sample = y_sample.view(-1).to(device)\n",
    "            logits = TextCNN(X_sample)\n",
    "            loss = criterion(logits, y_sample)\n",
    "            log.append(loss.item())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print('Epoch {}. Average loss {:.4f}'.format(i, np.mean(log)))\n",
    "        \n",
    "        if i == 10:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = 0.005\n",
    "                \n",
    "        if i == 20:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = 0.001\n",
    "        \n",
    "    print('\\nTesting\\n')\n",
    "    predictions = []\n",
    "    TextCNN.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for X_sample, _ in iter(test_loader):\n",
    "            \n",
    "            X_sample = X_sample.to(device)\n",
    "            logits = TextCNN(X_sample)\n",
    "            _, index = logits.topk(1, 1)\n",
    "            index = index.view(-1).numpy().tolist()\n",
    "            predictions += index\n",
    "    \n",
    "    y_test = y_test.reshape(-1).tolist()\n",
    "    y_test = [id2label[ind] for ind in y_test]\n",
    "    predictions = [id2label[ind] for ind in predictions]\n",
    "    \n",
    "    print('\\nTest result for {} :'.format(subject))\n",
    "    print(classification_report(y_test, predictions))\n",
    "    \n",
    "    return TextCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "Due to the training speed of CNN, only a small number of filters are used and only one example is provided.\n",
    "\n",
    "Besides, the learning schedule is changed. Using 0.001 is too small in the begin phase and 20 epochs will not be enough. The learning schedule is now 0.01, 0.005, 0.001 for 10 epochs each.\n",
    "\n",
    "To get better results, training with GPU with more filters will be necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/09/k_9rj22d0dgbjd8832nhvlbh0000gn/T/jieba.cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model cost 0.533 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Data\n",
      "Creating training/testing set\n",
      "Training\n",
      "\n",
      "Epoch 1. Average loss 1.0478\n",
      "Epoch 2. Average loss 0.8822\n",
      "Epoch 3. Average loss 0.6329\n",
      "Epoch 4. Average loss 0.4705\n",
      "Epoch 5. Average loss 0.4015\n",
      "Epoch 6. Average loss 0.3751\n",
      "Epoch 7. Average loss 0.3480\n",
      "Epoch 8. Average loss 0.3543\n",
      "Epoch 9. Average loss 0.3444\n",
      "Epoch 10. Average loss 0.3359\n",
      "Epoch 11. Average loss 0.3326\n",
      "Epoch 12. Average loss 0.3128\n",
      "Epoch 13. Average loss 0.2991\n",
      "Epoch 14. Average loss 0.3150\n",
      "Epoch 15. Average loss 0.3065\n",
      "Epoch 16. Average loss 0.3169\n",
      "Epoch 17. Average loss 0.3076\n",
      "Epoch 18. Average loss 0.2910\n",
      "Epoch 19. Average loss 0.3018\n",
      "Epoch 20. Average loss 0.3050\n",
      "Epoch 21. Average loss 0.2885\n",
      "Epoch 22. Average loss 0.2919\n",
      "Epoch 23. Average loss 0.2929\n",
      "Epoch 24. Average loss 0.2872\n",
      "Epoch 25. Average loss 0.2998\n",
      "Epoch 26. Average loss 0.2836\n",
      "Epoch 27. Average loss 0.2902\n",
      "Epoch 28. Average loss 0.2794\n",
      "Epoch 29. Average loss 0.2849\n",
      "Epoch 30. Average loss 0.2836\n",
      "\n",
      "Testing\n",
      "\n",
      "\n",
      "Test result for history :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         古代史       0.90      0.82      0.86       203\n",
      "         现代史       0.69      0.73      0.71       464\n",
      "         近代史       0.62      0.61      0.61       327\n",
      "\n",
      "   micro avg       0.71      0.71      0.71       994\n",
      "   macro avg       0.74      0.72      0.73       994\n",
      "weighted avg       0.71      0.71      0.71       994\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = train_TextCNN('history', [2, 2, 3, 3, 4, 4, 5, 6, 7], 0.1, epoch = 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
